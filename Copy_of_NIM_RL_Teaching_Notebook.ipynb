{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedAbraar302/aiml.ipynb/blob/main/Copy_of_NIM_RL_Teaching_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0ea81d7",
      "metadata": {
        "id": "d0ea81d7"
      },
      "source": [
        "# ðŸ§  Reinforcement Learning with the NIM Game\n",
        "Let's teach our AI how to win a simple game using Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2326000a",
      "metadata": {
        "id": "2326000a"
      },
      "source": [
        "## ðŸŽ® The NIM Game Rules\n",
        "- Start with 21 sticks.\n",
        "- Each player takes 1, 2, or 3 sticks on their turn.\n",
        "- The player who takes the **last stick loses**.\n",
        "\n",
        "We'll train an AI to get smarter over time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "d6b59538",
      "metadata": {
        "id": "d6b59538"
      },
      "outputs": [],
      "source": [
        "MAX_STICKS = 21\n",
        "ACTIONS = [1, 2, 3, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598d7dad",
      "metadata": {
        "id": "598d7dad"
      },
      "source": [
        "## ðŸ§  Step 1: Create a Q-table\n",
        "Weâ€™ll use a dictionary to store the AIâ€™s knowledge â€” the expected value (Q) of taking each action in every possible state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0ab88528",
      "metadata": {
        "id": "0ab88528"
      },
      "outputs": [],
      "source": [
        "Q = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c985115",
      "metadata": {
        "id": "3c985115"
      },
      "source": [
        "## ðŸŽ² Step 2: Action Choice\n",
        "Letâ€™s write a function that chooses an action. Weâ€™ll use **epsilon-greedy** â€” random at first, smarter later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e717bdea",
      "metadata": {
        "id": "e717bdea"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def choose_action(state, epsilon):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0 for a in ACTIONS}\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice([a for a in ACTIONS if a <= state])\n",
        "    return max(Q[state], key=Q[state].get)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d01dfae",
      "metadata": {
        "id": "9d01dfae"
      },
      "source": [
        "## ðŸ’¡ Step 3: Q-Value Update Rule\n",
        "Weâ€™ll update the Q-values using this formula:\n",
        "```\n",
        "Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s') - Q(s,a))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d3ed83b9",
      "metadata": {
        "id": "d3ed83b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_q(state, action, reward, next_state, alpha=0.1, gamma=0.9):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0 for a in ACTIONS}\n",
        "    if next_state not in Q:\n",
        "        Q[next_state] = {a: 0 for a in ACTIONS}\n",
        "    max_q_next = max(Q[next_state].values())\n",
        "    Q[state][action] += alpha * (reward + gamma * max_q_next - Q[state][action])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5302fc2c",
      "metadata": {
        "id": "5302fc2c"
      },
      "source": [
        "## ðŸ” Step 4: Training Loop\n",
        "Now weâ€™ll play lots of games where the AI learns from experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "2fe1d694",
      "metadata": {
        "id": "2fe1d694"
      },
      "outputs": [],
      "source": [
        "def train(episodes=10000, epsilon=0.3, alpha=0.1, gamma=0.9):\n",
        "    for _ in range(episodes):\n",
        "        state = MAX_STICKS\n",
        "        last_state, last_action = None, None\n",
        "\n",
        "        while state > 0:\n",
        "            action = choose_action(state, epsilon)\n",
        "            next_state = state - action\n",
        "\n",
        "            if last_state is not None:\n",
        "                update_q(last_state, last_action, 0, state, alpha, gamma)\n",
        "\n",
        "            last_state = state\n",
        "            last_action = action\n",
        "\n",
        "            if next_state == 0:\n",
        "                update_q(state, action, -1, next_state, alpha, gamma)\n",
        "                break\n",
        "\n",
        "            valid_opponent_actions = [a for a in ACTIONS if a <= next_state]\n",
        "            if not valid_opponent_actions:\n",
        "                update_q(last_state, last_action, 0, next_state, alpha, gamma)\n",
        "                break\n",
        "\n",
        "            opponent_action = random.choice(valid_opponent_actions)\n",
        "            state = next_state - opponent_action\n",
        "\n",
        "            if state <= 0:\n",
        "                update_q(last_state, last_action, 1, next_state, alpha, gamma)\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63bf4e99",
      "metadata": {
        "id": "63bf4e99"
      },
      "source": [
        "## ðŸš€ Train the AI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "0c4b8ebe",
      "metadata": {
        "id": "0c4b8ebe"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGmkPcQFjQt5",
        "outputId": "fdc11206-4294-4231-e751-eca0c113b589"
      },
      "id": "nGmkPcQFjQt5",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{21: {1: 0.6949143256485893, 2: 0.7272320476749335, 3: 0.7326054095980893, 4: 0.7498745346339808}, 19: {1: 0.7396839740365225, 2: 0.4281240019659795, 3: 0.45526162097351014, 4: 0.5333557013845269}, 16: {1: 0.7692375450321817, 2: 0.7866406225030995, 3: 0.79626159651319, 4: 0.8270502820495111}, 11: {1: 0.8634540641588887, 2: 0.8645265566855302, 3: 0.8610683415746269, 4: 0.9297580955763165}, 9: {1: 0.8775716290136331, 2: 0.9171777940792544, 3: 0.9145278397640869, 4: 0.677880752295148}, 7: {1: 0.9076593056284399, 2: 0.5859005237391894, 3: 0.9785521105911106, 4: 0.7092741564284064}, 2: {1: 0.9999999999999996, 2: -0.9999999999999608, 3: 0, 4: 0}, 1: {1: -0.9999999999999996, 2: 0.0, 3: 0, 4: 0}, 12: {1: 0.8306826711252127, 2: 0.8458334498733349, 3: 0.8482133543151322, 4: 0.862835887151084}, 18: {1: 0.7556815647336532, 2: 0.7156525996961419, 3: 0.7251536550563824, 4: 0.7974056646823325}, 15: {1: 0.8053850106369571, 2: 0.8064362607379042, 3: 0.8173363776063043, 4: 0.8284721351391358}, 3: {1: 0.5973612156829735, 2: 0.9999999999999996, 3: -0.9999999999781978, 4: 0}, 0: {1: 0, 2: 0, 3: 0, 4: 0}, 17: {1: 0.7594781309535721, 2: 0.7852935383289361, 3: 0.7830128558467947, 4: 0.79878859152589}, 13: {1: 0.8162072015433647, 2: 0.8274325135964259, 3: 0.8243664070524988, 4: 0.8640047504381481}, 10: {1: 0.8570053320902948, 2: 0.8581554206787819, 3: 0.882249649471024, 4: 0.9420107275457024}, 6: {1: 0.6382634370943983, 2: 1.079685417179568, 3: 0.7788620254221924, 4: 0.7430820728215989}, -1: {1: 0, 2: 0, 3: 0, 4: 0}, 14: {1: 0.8101956290343272, 2: 0.8097997887722448, 3: 0.8264595470962619, 4: 0.8125027050613591}, 4: {1: 0.9359803701844301, 2: 0.7996135894167697, 3: 0.9999999999999996, 4: -0.9999991791689895}, 8: {1: 0.8858768528794292, 2: 0.9402839552769922, 3: 0.6623733456427249, 4: 0.814733357066227}, 5: {1: 0.9037098233091703, 2: 0.680819504906004, 3: 0.9558361747253185, 4: 0.9999999999999996}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c30e744",
      "metadata": {
        "id": "2c30e744"
      },
      "source": [
        "## ðŸ§ª Letâ€™s play against the AI!"
      ]
    },
    {
      "source": [
        "def play():\n",
        "    state = MAX_STICKS\n",
        "    while state > 0:\n",
        "        print(f\"Sticks left: {state}\")\n",
        "        move = int(input(\"Your move (1â€“3): \"))\n",
        "        state -= move\n",
        "        if state <= 0:\n",
        "            print(\"You took the last stick. You lose!\")\n",
        "            return\n",
        "\n",
        "        valid_ai_moves = [a for a in ACTIONS if a <= state]\n",
        "        if valid_ai_moves:\n",
        "            if state in Q:\n",
        "                # Find the action with the minimum Q-value (the \"worst\" move)\n",
        "                ai_move = min(Q[state], key=Q[state].get)\n",
        "\n",
        "                # Make sure the chosen \"worst\" move is actually valid from the current state\n",
        "                if ai_move not in valid_ai_moves:\n",
        "                     ai_move = random.choice(valid_ai_moves) # Fallback to random if worst move isn't valid\n",
        "            else:\n",
        "                # If the state is not in Q, just choose a valid move randomly\n",
        "                ai_move = random.choice(valid_ai_moves)\n",
        "\n",
        "            print(f\"AI takes {ai_move} stick(s).\")\n",
        "            state -= ai_move\n",
        "            if state <= 0:\n",
        "                print(\"AI took the last stick. You win!\")\n",
        "                return\n",
        "        else:\n",
        "            print(\"AI has no valid moves left. You win!\")\n",
        "            return"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dQYWCNXhC1g8"
      },
      "id": "dQYWCNXhC1g8",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e912145f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e912145f",
        "outputId": "a1575cbf-2a8d-4630-e2bb-4cd8ff8b94fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sticks left: 21\n",
            "Your move (1â€“3): 3\n",
            "AI takes 2 stick(s).\n",
            "Sticks left: 16\n",
            "Your move (1â€“3): 2\n",
            "AI takes 2 stick(s).\n",
            "Sticks left: 12\n",
            "Your move (1â€“3): 2\n",
            "AI takes 1 stick(s).\n",
            "Sticks left: 9\n",
            "Your move (1â€“3): 3\n",
            "AI takes 1 stick(s).\n",
            "Sticks left: 5\n",
            "Your move (1â€“3): 2\n",
            "AI takes 3 stick(s).\n",
            "AI took the last stick. You win!\n"
          ]
        }
      ],
      "source": [
        "play()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc857d07",
      "metadata": {
        "id": "cc857d07"
      },
      "source": [
        "## ðŸŽ‰ Summary\n",
        "You just trained an agent to play a game using trial-and-error. Thatâ€™s the magic of Reinforcement Learning!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language": "python",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}